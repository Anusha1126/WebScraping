# -*- coding: utf-8 -*-
"""Untitled4.ipynb



Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k0jrPo3BFh7OiQDPGTHd-fWg3J16j6Qd
"""


import csv # Module helps to write the data into the file
import requests # To fetch the URL and perform read data from URL to use get
import os
from bs4 import BeautifulSoup # Parse the HTML Content
import re # Filter the data whether satisfies required set of conditions

def create_dataframe():
    # URL of the eBay search page for new books
    my_url = "https://www.ebay.com/sch/i.html?_nkw=books&rt=nc&LH_ItemCondition=1000"
    # Fetch the web page content
    result = requests.get(my_url)
    # Parse the HTML content using BeautifulSoup
    soup = BeautifulSoup(result.text, 'html.parser')
    # print(soup)

    # List to store product data
    product_list = []
    # Find all product listings
    products = soup.find_all('li', class_='s-item')

    # Loop through each product and extract the title and price, skip the first and take next 10
    for idx, product in enumerate(products[1:11]): # Enumerate is an object which generates pair of an index and sliced list
        try:
            # Extract the product title
            title_element = product.find('h3', class_='s-item__title')
            if title_element:
                productTitle = title_element.text.strip()# Removes leading and trailing spaces
            else:
                title_element = product.find('div', class_='s-item__title')
                productTitle = title_element.text.strip() if title_element else 'No title found'

            # Extract the product price
            price_element = product.find('span', class_='s-item__price')
            get_price = price_element.text.strip() if price_element else '0'
            productPrice = re.sub(r"[$,]", "", get_price)

            # Append the data to the product list
            product_list.append([productTitle, productPrice])
            print(f"Product {idx+1}: Title: {productTitle}, Price: {productPrice}")  # Debug print

        except Exception as e:
            print(f"Error parsing product {idx+1}: {e}")
            continue

    return product_list

def writefile_CSV(df, filename):
    try:
        # Write the product data to a CSV file
        with open(filename, 'w', newline='', encoding="utf-8") as f: # With-memory management ensures file is closed properly and unifed format
            csv_row = ['Title', 'Price']
            w = csv.writer(f) # Creates the writer object that will write to the file
            w.writerow(csv_row)
            w.writerows(df)
    except Exception as e:
        print(f"Error writing to CSV file: {e}")

def main():
    # Create the dataframe by scraping data
    df = create_dataframe()
    print("Scraped Data:")
    print(df)  # Print scraped data to check

    # Absolute paths to avoid permission issues
    script_dir = os.path.dirname(os.path.abspath(__file__))
    print(script_dir)
    products_file ='products.csv'
    full_driver_path = os.path.join(script_dir, products_file)
    products_file=full_driver_path

    # Write the dataframe to a CSV file
    if df:
        writefile_CSV(df, products_file)
        print(f"Data has been written to {products_file}")
    else:
        print("No data scraped.")

if __name__ == "__main__":
    main()